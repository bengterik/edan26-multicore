labb 6: HW Transactional memory
Med atomic+barrier-lösningen var det endast att ändra de atomära instruktionerna till transaktioner. En transaktion blir atomär då den när den är klar försäkrar sig att ingen annan transaktion gjorts klart under tiden. Om en annan transaktion blivit klar under tiden den körts görs instruktionerna i transaktionsblocket om tills de är först med att bli klara.l 

Labb 5: Rust
Ownership: a single variable is always the owner of some memory
Moving: the ownership can be moved to new variables, invalidating the older reference
Borrowing: the memory can be borrowed to functions and other values with the use of the &-operator.
ARC: The arc is a type of atomic wrapper for any object or data. A thread can clone() an ARC which allows the thread to modify the ARC while also "listening" for changes to it.

Labb 4
Med vanlig int ~8 sekunder medans med atomic ~14 sekunder.

1. Instruktionerna 
* isync: "The isync and ics instructions cause the processor to refetch any instructions that might have been fetched prior to the isync or ics instruction."
* hwsync: ensures sequential consistency on power 

2. 
Move -=/+= to first phase

3. 
Sequential consistency, although the compound operators are not atomic.

4. 
With sequential consistency all memory operations before the load/store operation has to be visible to the other threads before it can move on. With a relaxed memory model this is not a requirement and the values of the variables does not have to be consistent over the threads.

The change of the two atomic variables are not dependent on other shared variables, making reordering of those operations not important to the correctness of the algorithm. 

5.
Running it with the relaxed memory model seems a tiny bit faster. This could be due that the two atomic operations

    atomic_fetch_sub_explicit(&u->coming_flow, d, memory_order_relaxed);
    atomic_fetch_add_explicit(&v->coming_flow, d, memory_order_relaxed);

does not have to wait for the other operation to be updated in the memory of the other threads.

7. 
First I started out using operations as in lab three but then went for iterating over all the nodes. This frees the many allocations that had to be done for the operations.

Lab 3
Frågor:
- realloc, segmentation fault
- dubbla relabels


- Varje tråd har en lista med noder 
1. huvud-tråden:
    - delar ut alla med excess till listor
    - blockera på fas 1 barrier 

2: alla trådar
    - trådarna går igenom sin lista och registrerar vilka pushes alternativt relables som ska göras
    - blockera på fas 1 barrier
    - blockera på fas 2 barrier

3: huvud-tråden
    - utför operationerna i varje tråds operationskö
    - om s->e == t->e då g->klar
    - blockera på fas 2 barrier

4: alla trådar
    - om g->klar då break 
    - annars steg 2


Lab 2
Maila Jonas
Redovisat för David Engström

1. Data races and deadlocks 
The entire graph has a lock that protects:
- entering excess
- leaving excess
- relabeling

Then, each node has lock for when pushes are made to and from that node.

Could they be more granular?

For the node locks an order u < v is enforced to avoid deadlocks.

2. Load balancing
In this implementation, each thread will pick from the excess queue, and if the queue is empty the thread will terminate. At first I thought this would create very uneven workloads between the threads, but when examining how many nodes each thread got it was not that uneven. I tried to implement signaling with each thread having its' own work queue and a thread dedicated to handing out work. Ended up too complex..

3. Faster than sequential?
No, not at all...

4. C vs JVM
C was faster, 32 vs 55 sec.

5. Source codes

6. Improvements
Other than load balancing I was not sure what to improve. 
Ideas:
- partition the graph so that the lock contention for a single node decreases
- divide the work in rounds, do pushes and relabels seperately? 

